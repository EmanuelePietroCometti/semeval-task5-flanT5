# SemEval Task 5: Plausibility estimation with Flan-T5-XL

## Reproducibility

### Local Execution

Hardware Prerequisites:
NVIDIA GPU with at least **16GB VRAM** (required for Flan-T5-XL with QLoRA).

***Note:*** For GPUs with less memory (e.g., T4 16GB or lower), we recommend reducing the `batch_size` in `config/config.yaml` or using the Flan-T5-Large variant.

1. Clone and Install

```Bash
git clone https://github.com/your-username/semeval-task5-flant5.git
cd semeval-task5-flant5

# Create a virtual environment (recommended)
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

2. Data Preparation:

   1. Download the official dataset from the SemEval 2026 Task 5 competition page. ([SemEval 2026 Task 5](https://nlu-lab.github.io/semeval.html))
   2. Create a `data/` directory in the root of the project.
   3. Place the JSON files inside `data/` and rename them for consistency:
      * train.json
      * dev.json
      * test.json
3. Start Training To start fine-tuning using the parameters defined in config/config.yaml:

```Bash
python scripts/train.py
```

The model artifacts (LoRA adapters) will be saved in the `outputs/` directory.

### Google Colab Execution

For an immediate experience without local configuration, you can use the provided Jupyter notebook.

1. Upload the `launcher.ipynb` file to Google Colab.
2. Ensure you select a GPU Runtime (Menu: Runtime > Change runtime type > T4 GPU or A100 GPU).
3. Execute the cells in sequence: the notebook handles cloning the repository, installing dependencies, and launching the training process.

***Note:*** If you are using a standard T4 GPU (16GB), you may need to enable gradient checkpointing or reduce the batch size in the config file if you encounter OOM (Out Of Memory) errors.

### Advanced Configuration

All training hyperparameters (learning rate, batch size, LoRA rank, etc.) are fully configurable via the configuration file:

`config/config.yaml`

Modify this file to test different experimental setups without altering the source code.

## Introduction

This repository contains the code and resources for fine-tuning **Flan-T5-XL (3B parameters)** to solve SemEval Task 5. The task involves estimating the plausibility of a specific word sense (homonym) within a narrative context.

The problem is modeled as a **Regression via Classification** task. Instead of treating this strictly as a regression problem (outputting a single float) or a standard classification problem (outputting a class index), we leverage the semantic understanding of a sequence-to-sequence model. We force the model to output a probability distribution over the tokens `1`, `2`, `3`, `4`, `5` and compute the **Expected Value** of this distribution to obtain a continuous score.

This approach allows us to utilize the pre-trained knowledge of Flan-T5 while satisfying the continuous nature of the plausibility scores provided in the dataset.

## Previous attempts and challenges

Before arriving at the current optimized architecture, several experiments were conducted.

### The "Large" Variant

Initial experiments were conducted using `google/flan-t5-large` (780M parameters). Despite hyperparameter tuning, the model struggled to capture the subtle semantic nuances required to distinguish between slightly plausible and very plausible contexts. The move to the **XL (3B)** variant provided the necessary capacity for this semantic disambiguation.

### The KL-Divergence Approach

An earlier iteration of the loss function attempted to align the model's output distribution directly with the distribution of votes from the human annotators using **Kullback-Leibler (KL) Divergence**.

* **Hypothesis:** By teaching the model the full distribution of human uncertainty, it would learn to generalize better.
* **Outcome:** This approach failed. The model became "confused," struggling to optimize for the mean score while simultaneously trying to match the variance of the annotators. The gradients became noisy, preventing convergence on the primary metric (Spearman correlation). This led to the decision to switch to a **Weighted MSE** approach that treats annotator variance as a weighting factor rather than a target.

## Loss function and optimization

To handle the subjective nature of the task, we designed a custom hybrid loss function. The training objective combines **Cross-Entropy (CE)** to enforce syntactic correctness (predicting valid tokens) and **Uncertainty-Weighted Mean Squared Error (MSE)** to optimize the semantic regression score.

### Mathematical formulation

Let $p \in \mathbb{R}^5$ be the softmax probability distribution over the tokens $1, 2, 3, 4, 5$ generated by the model. The continuous predicted score $\hat{y}$ is the expected value:

$$
\hat{y} = \mathbb{E}[p] = \sum_{k=1}^{5} k \cdot p_k

$$

The total loss $\mathcal{L}$ is defined as:

$$
\mathcal{L} = \lambda_{CE} \cdot \mathcal{L}_{CE} + \lambda_{MSE} \cdot \mathcal{L}_{WMSE}

$$

Where:

* **$\mathcal{L}_{CE}$** is the standard Cross-Entropy loss on the target token (rounded integer).
* **$\mathcal{L}_{WMSE}$** is the Weighted Mean Squared Error, weighted by the inverse of the annotator standard deviation **$\sigma$**:

The Weighted Mean Squared Error (WMSE) accounts for annotator uncertainty ($\sigma_i$) and is calculated as:

$$
\mathcal{L}_{WMSE} = \frac{1}{N} \sum_{i=1}^{N} \frac{(\hat{y}_i - y_i)^2}{\sigma_i + \epsilon}

$$

where $\epsilon = 0.5$ is a smoothing term.

The total loss $\mathcal{L}$ combines the cross-entropy loss (structural) and the regression loss:

$$
\mathcal{L} = \lambda_{CE} \cdot \mathcal{L}_{CE} + \lambda_{MSE} \cdot \mathcal{L}_{WMSE}

$$

In this way, samples where human annotators disagreed (high $\sigma$) are penalized less, as the "ground truth" is inherently fuzzy. Samples with high agreement (low $\sigma$) heavily penalize the model for errors.

## Model Configuration: LoRA and quantization

To fine-tune the 3B parameter model on limited hardware (e.g., A100/L4 GPUs), we employed **QLoRA (Quantized Low-Rank Adaptation)**.

### Quantization

The base model is loaded using `bitsandbytes` with **4-bit NormalFloat (NF4)** quantization to minimize memory footprint while retaining dynamic range.

* `bnb_4bit_compute_dtype`: `bfloat16` (for stable gradients)
* `bnb_4bit_use_double_quant`: `True`

### LoRA configuration

We inject trainable rank decomposition matrices into the attention and feed-forward layers.

* **Rank (**$r$**):** 16
* **Alpha (**$\alpha$**):** 32 (Scaling factor **$\alpha/r = 2$**)
* **Dropout:** 0.1
* **Target Modules:** `["q", "v", "wi_0", "wi_1", "wo"]`

*Note:* We explicitly target `wi_0` and `wi_1` (part of T5's GatedGelu Feed-Forward network) in addition to the attention queries (`q`) and values (`v`), ensuring the model can adapt its reasoning capabilities, not just its attention mechanism.

## Prompt Strategy

Standard Large Language Models are trained with **causal attention** , meaning a token can only attend to previous tokens, not future ones. In a standard prompt (Context **$\to$** Question), the initial tokens (the context story) are processed without "knowing" the specific question or constraints that appear at the end.

As demonstrated by Leviathan et al. (2025), repeating the prompt effectively mitigates this limitation. By appending a second copy of the prompt, the tokens in the second repetition can attend to the entire first copy. This simulates bidirectional attention over the prompt, allowing the model to process the story (in the second pass) while fully aware of the constraints and the specific homonym sense defined in the first pass. **This technique has been proven to significantly improve performance on non-reasoning tasks without increasing generation latency, as the repetition only affects the pre-fill stage**.

We utilize a **Prompt Repetition** strategy to enhance the model's performance on the regression task. The input structure is:

```bash
query = (
        f"Task: Rate the plausibility of the word sense for the homonym in the story.\n"
        f"Scale: 1 (not plausible) to 5 (very plausible).\n\n"
        f"Story: {story}\n"
        f"Homonym: {homonym}\n"
        f"Sense to evaluate: {judged_meaning}\n\n"
        f"Constraint: Respond only with a single integer between 1 and 5.\n"
        f"Answer: "
    )

 # Prompt repetition
 full_prompt = f"{query}\nLet me repeat: {query}"
```

## Custom implementation details

To solve the unique challenges of this task—specifically the need for regression on a generative model and handling subjective human labels—we implemented several custom components that override the standard HuggingFace `Trainer` workflow.

### Custom metrics

Standard regression metrics like MSE or MAE are insufficient for this task because they do not account for the inherent subjectivity of the data (represented by annotator standard deviation) nor the importance of relative ranking in semantic plausibility. We implemented a custom `compute_metrics` function that calculates two complementary scores:

1. **Accuracy within standard deviation**
2. **Spearman correlation**

#### **Accuracy within standard deviation (Acceptability)**

This metric measures whether the model's prediction is "indistinguishable" from a human judgment. Since the ground truth is an average of multiple annotators, a prediction is considered correct if it falls within the range of human disagreement.

$$
\text{Accuracy} = \frac{1}{N} \sum_{i=1}^{N} \mathbb{I}\left( | \hat{y}_i - y_i | \le \max(1.0, \sigma_i) \right)

$$

It prevents penalizing the model for "errors" that are actually statistically valid given the noise in the dataset.

### Spearman's Rank Correlation (Monotonicity)

While accuracy measures "closeness," it doesn't capture whether the model correctly ranks contexts from least plausible to most plausible. Spearman's correlation ($\rho$) assesses the monotonic relationship between the predicted ranks ($R(\hat{y})$) and the true ranks ($R(y)$):

$$
\rho = \frac{\text{cov}(R(\hat{y}), R(y))}{\sigma_{R(\hat{y})} \sigma_{R(y)}}

$$

In many downstream applications, the absolute score (e.g., 3.4 vs 3.6) matters less than the model's ability to correctly say that *Sentence A* is more plausible than *Sentence B*. A high Spearman score indicates the model has learned the correct semantic ordering.

#### **Combined Score for Model Selection:**

We optimize for a weighted average:

`combined_score` = **$0.7 \times \text{Accuracy} + 0.3 \times \text{Spearman}$**

### RobustDataCollator

The standard HuggingFace `DataCollatorForSeq2Seq` is designed to handle only `input_ids`, `attention_mask`, and `labels` (token IDs). It automatically pads these sequences but typically ignores or discards additional numerical columns present in the dataset.

Our custom loss function (`ExpectedValueTrainer`) requires access to continuous numerical data—specifically `target_scores` (the float average) and `stdev` (the standard deviation)—during the training step to compute the **Weighted MSE**.

The **`RobustDataCollator`** bridges this gap:

1. **Preservation:** It explicitly intercepts the `target_scores` and `stdev` fields from the dataset samples before they are batched.
2. **Tensor Conversion:** It converts these lists of floats into PyTorch tensors (`dtype=torch.float32`).
3. **Integration:** It injects these tensors back into the batch dictionary alongside the tokenized inputs.
   Without this custom collator, the `compute_loss` method would receive only the tokenized text, making it impossible to calculate the regression component of the loss or apply the uncertainty weighting.

### Custom prediction step

We overrode the `prediction_step` in `ExpectedValueTrainer` to fundamentally change how the model generates outputs during evaluation.

`Seq2SeqTrainer` typically calls `model.generate()`, which outputs discrete token IDs (e.g., the token "4").

We perform a forward pass to get the raw logits. We then apply a softmax over the specific indices corresponding to tokens `1`, `2`, `3`, `4`, `5` and calculate the **expected value** (**$\sum p_i \cdot i$**).

This returns a continuous float (e.g., `3.42`) instead of an integer. This allows us to compute the Spearman correlation on the continuous manifold, providing a much more granular evaluation of the model's semantic understanding than discrete classification would allow.

## Experimental results

We conducted **multiple** runs to iteratively improve performance. Below is a brief summary of the key points from the training process.


| Version | Model         | Accuracy within std | Spearman | p-Value     | Note                   |
| :-------- | --------------- | --------------------- | ---------- | :------------ | ------------------------ |
| v1.0.0  | Flan-T5-Large | 0.5215              | 0.6626   | 1.3040e-118 | Results from codabench |
| v2.0.0  | Flan-T5-Large | 0.7602              | 0.5711   | 1.2504e-81  | Results from codabench |
| v3.0.0  | Flan-T5-Large | 0.7516              | 0.5865   | 4.8871e-87  | Results from codabench |
| v4.0.0  | Flan-T5-Large | 0.7466              | 0.5765   | -           |                        |
| v5.0.0  | Flan-T5-Large | 0.7245              | 0.5762   | -           |                        |
| v6.0.0  | Flan-T5-XL    | 0.8107              | 0.6841   | 2.4397e-129 | Results from codabench |
| v7.0.0  | Flan-T5-XL    | 0.7785              | 0.6612   | 5.8541e-118 | Results from codabench |

### Main differences between versions


| Version    | Base Model & Quantization | LoRA Configuration                             | Loss Function Strategy                                                        | Key Changes & Observations                                                                                                                                                                     |
| :----------- | :-------------------------- | :----------------------------------------------- | :------------------------------------------------------------------------------ | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **v1.0**   | Flan-T5 Large (FP32)      | Rank: 16<br>Targets: q, v, wi, wo, lm_head     | **Hybrid Regression**<br>0.4 CE + 0.6 MSE<br>(Standard MSE on expected value) | Treats the task as a regression problem using token probabilities. High weight on MSE forces the model to approximate the score directly.                                                      |
| **v2.0.0** | Flan-T5 Large (FP32)      | Rank: 16<br>Targets: q, v, wi, wo, lm_head     | **Gaussian Soft Labels**<br>0.7 KL Div + 0.3 wMSE<br>(Distribution Matching)  | Models targets as Gaussian distributions based on annotator stdev. Prioritizes learning the "shape" of human uncertainty (KL > MSE).                                                           |
| **v3.0.0** | Flan-T5 Large (FP32)      | Rank: 16<br>Targets: q, v, wi, wo, lm_head     | **Gaussian Soft Labels**<br>0.7 KL Div + 0.3 wMSE                             | Identical training logic to v2.0. Likely focuses on code refactoring and pipeline stability (e.g., logging improvements) without altering the math.                                            |
| **v4.0.0** | Flan-T5 Large (FP32)      | Rank: 16<br>Targets: q, v, wi, wo, lm_head     | **Gaussian Soft Labels**<br>0.7 KL Div + 0.3 wMSE                             | Increases tokenizer max_length to 1024. Addresses truncation issues for longer inputs while maintaining the v2/v3 loss strategy.                                                               |
| **v5.0.0** | Flan-T5 Large (FP32)      | Rank: 16<br>Targets: q, v, wi, wo, lm_head     | **Gaussian Soft Labels**<br>0.3 KL Div + 0.7 wMSE<br>(Weight Flip)            | **Inverts the loss weights. Shifts focus from matching the distribution shape (KL) to minimizing the point-wise error (MSE), acting as a "correction" to v2-v4.                                |
| **v6.0.0** | Flan-T5 XL (8-bit Int8)   | Rank: 16<br>Targets: q, v, wi, wo, lm_head     | **Weighted Regression**<br>0.1 CE + 0.9 wMSE<br>(Back to Regression)          | Major upgrade to XL model using 8-bit quantization. Abandons KL Divergence. Uses aggressive MSE weighting (0.9) with higher smoothing (stdev + 0.5) to stabilize training on the larger model. |
| **v7.0.0** | Flan-T5 XL (4-bit NF4)    | Rank: 16<br>Targets: q, v, wi, wo (No lm_head) | **Weighted Regression**<br>0.1 CE + 0.9 wMSE                                  | Optimizes v6 by using 4-bit NF4 quantization for lower memory footprint. Removes lm_head from LoRA targets, reducing trainable parameters while maintaining the regression focus.              |

## References and scientific basis

This project relies on several key papers and findings in the NLP field:

1. *Raffel, C., et al. (2020). "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"*: [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683https:/)
2. *Chung, H. W., et al. (2022). "Scaling Instruction-Finetuned Language Models":* [Scaling Instruction-Finetuned Language Models](https://arxiv.org/pdf/2210.11416)
3. *Leviathan, Y., Kalman, M., & Matias, Y. (2025). "Prompt Repetition Improves Non-Reasoning LLMs":* [Prompt Repetition Improves Non-Reasoning LLMs](https://arxiv.org/pdf/2512.14982https:/)
4. *Hu, E. J., et al. (2021). "LoRA: Low-Rank Adaptation of Large Language Models":* [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/pdf/2106.09685https:/)
5. Dettmers, T., et al. (2023). *"QLoRA: Efficient Finetuning of Quantized LLMs"*: [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/pdf/2305.14314https:/)
